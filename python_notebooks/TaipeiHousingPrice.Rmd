---
title: "Predicting Housing Price in Taipei"
date: "4/28/2025"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(progress = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE
)

library(reticulate)
py_config()
library(keras)
library(tensorflow)
tf$get_logger()$setLevel("ERROR")

library(dplyr)
library(MASS)
library(ISLR2)
library(glmnet)
library(caret)
library(readxl)
library(ggplot2)
library(randomForest)
library(iml)
```


```{r, warning=FALSE,message=FALSE, include=FALSE}
db=read_excel('real.xlsx')

db <- db %>% dplyr::select(-No)
```

```{r, warning=FALSE,message=FALSE, include=FALSE}

db<-db %>% 
  rename(date=`X1 transaction date`)

db<-db %>% 
  rename(age=`X2 house age`)

db<-db %>% 
  rename(distance=`X3 distance to the nearest MRT station`)

db<-db %>% 
  rename(storenum=`X4 number of convenience stores`)

db<-db %>% 
  rename(latitude=`X5 latitude`)

db<-db %>% 
  rename(longitude=`X6 longitude`)

db<-db %>% 
  rename(price=`Y house price of unit area`)

```

# Executive Sumamry
## Study Goal
The goal of this study is to predict the housing price per unit area in Taipei City using different predictive models and available housing features. Accurate predictions generated by these models can not only assist potential buyers and sellers but also inform urban planning, lending decisions, time trends, and investment analysis for companies and organizations in the industry.

## Data Description
The dataset used in this study was collected by Cheng Yeh in 2018 and is publicly available on Kaggle (https://www.kaggle.com/datasets/hastingssibanda/taipei-housing-dataset-uci). This dataset contains information on 414 houses located in Taipei City, Taiwan, with seven variables. The response variable, Price, represents the housing price per unit area measured in ten thousand New Taiwan Dollars. The independent variables include factors such as transaction date, house age, distance to the nearest MRT station, transaction date, number of convenience stores nearby, latitude, and longitude. We selected these variables because they are commonly recognized as key drivers of housing prices in urban real estate markets.

## Methods Used
For the prediction task, we implemented several models to explore different predictive approaches. We began with a vanilla linear regression as the baseline model and then applied relaxed lasso regression, neural networks, and random forests. Each of these models offers different advantages in terms of flexibility, interpretability, and the ability to handle complex patterns or non-linear relationships in the data.

To assess and compare the models, we used Testing Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) as the primary evaluation metrics. These metrics provide a quantitative measure of the average magnitude of prediction errors, with RMSE offering a more interpretable value in the same units as the target variable. The data was divided into a training set (80% of data) and a testing set (20% of data) to ensure that model evaluation reflects real-world predictive performance and to prevent overfitting.

## Findings
In addition to evaluating predictive accuracy, we also analyzed the influence of different variables on housing prices. By examining the structure of the regression models and decision trees, we gained valuable insights into how factors such as proximity to public transport, property age, and geographical coordinates contribute to price variations.

- **Baseline Linear Regression** explained approx. 62.53% of variance and achieved RMSE $\approx$ 7.75, identifying date, age, distance, store count, and latitude as significant predictors.
- **Relaxed LASSO** selected four key predictors (age, distance, store count, latitude) and yielded RMSE $\approx$ 7.78, overall similar to the baseline.
- **Neural Network** (two hidden layers of 64 $\to$ 32 units, 30% dropout rate, and early stopping) captured non-linear relationships to reduce error to RMSE $\approx$ 6.36, a roughly 20% improvement over linear models. Permutation Importance on the neural net highlighted distance to MRT as overwhelmingly the most critical feature.
- **Random Forest** further reduced test RMSE to $\approx$ 5.83, edging out the neural net. Its variable‐importance plot again ranks distance to MRT and age at the very top, with latitude and date contributing moderately and store count playing a minor role.

Conclusion: All non‐linear models deliver clear improvements over the linear approaches: the neural network cuts average error from an RMSE of 7.75 to 6.36, while the random forest pushes performance further to an RMSE of 5.83. Both agree that proximity to MRT is the dominant driver, followed by housing age, with latitude and transaction date playing secondary roles and store count only a minor one. If we were to choose a model, the random forest edges out the neural net on raw accuracy, making it the top pick when minimizing prediction error is paramount. The neural network, however, remains attractive for its smoother training dynamics and straightforward extension to more complex data or additional features. Overall, this study aims not only to identify the best-performing predictive model for Taipei housing prices but also to provide interpretive insights that can assist stakeholders in making informed decisions in the housing market.

\newpage

# Detailed Analysis
## Data Descriptions

The dataset was collected by Cheng Yeh in 2018 and can be found in https://www.kaggle.com/datasets/hastingssibanda/taipei-housing-dataset-uci. This dataset contains information on the houses in Taipei City, Taiwan. There are in total 414 houses documented in the dataset. The response variable “Price” represents housing price per unit area in ten thousand New Taiwan Dollars. The area is measured in the local unit “Ping”. 1 Ping is roughly equivalent to 3.3 square meters. There are in total six predictor variables. The variables include House Age, Distance to nearest MRT station, Transaction Date, Number of convenience stores nearby, Latitude, and Longitude. The transaction date variable is intentionally transformed by the author as 201x.xxx to serve as a pseudo-continuous variable. 201x represents the year transaction took place. The decimal represents the corresponding month divided by 12. For instance, 2012.500 would be June 2012. If the month is December, the decimal is zero, and one year is added to the integer part. For instance, 2013.000 would be December 2012. Also, there are no missing values.
 


## EDA
```{r, fig.height=6, fig.width=8, echo=FALSE}
par(mfrow=c(2,3))  

hist(db$age, main="Age of House", col="lightblue", xlab="Years")
hist(db$distance, main="Distance To MRT", col="lightblue", xlab="Meters")
hist(db$storenum, main="Number of Stores Nearby", col="lightblue", xlab="Count")
hist(db$latitude, main="Latitude", col="lightblue", xlab="Latitude")
hist(db$longitude, main="Longitude", col="lightblue", xlab="Longitude")
hist(db$price, main="Price Per Unit Area", col="lightblue", xlab="Price")
```


```{r}
summary(db %>% dplyr::select(-date))
```
```{r, include= FALSE}
min(db$date)
max(db$date)
```

The histograms (frequency) and data summary for all variables except for date have been displayed. The date ranges from August 2012 to July 2013.

House ages span from brand-new to about 44 years (median $\approx$ 16 yrs), while distances to the nearest MRT range widely from $\sim$ 23 m to $\sim$ 6500 m (median $\sim$ 492 m, mean $\sim$ 1 084 m), both showing right-skew. The number of nearby convenience stores runs 0–10 (median 4), fairly symmetric around its mean ($\sim$ 4.1). Latitude (24.93–25.01, median 24.97) and longitude (121.48–121.56, median 121.50) vary only slightly, reflecting a compact urban area. Prices per ping range 7.6–117.5 (median 38.45, mean 37.98), with a moderate spread and a few high-end outliers. We decide to drop the case with the highest price since it is an outlier.

```{r}
db <- db[db$price != 117.5, ]

set.seed(123)  # for reproducibility
library(caret)

# Create data partition (80% training, 20% testing)
split <- createDataPartition(db$price, p = 0.8, list = FALSE)

# Split the data
train_set <- db[split, ]
test_set <- db[-split, ]
```

# In-depth analysis
## Vanilla Linear Regression (Baseline Model)

```{r}
train_lr <- train_set
test_lr <- test_set
```
```{r}
fit1 <- lm(price~. , data = train_lr)
summary(fit1)
```
```{r, echo = FALSE}
predictions <- predict(fit1, newdata = test_lr)
# Calculate errors
mse <- mean((test_lr$price - predictions)^2)
rmse <- sqrt(mse)

# Print errors
cat("Test MSE:", round(mse, 2), "\n")
cat("Test RMSE:", round(rmse, 2), "\n")
```

Our team first fits a multiple linear regression model as the baseline model. The multiple R-squared is 0.6253, suggesting that 62.53% of the variance in housing price is explained by the model. At 0.05 level, the significant variables include transaction date, housing age, distance to nearest MRT station, number of store, and latitude. (As stated before, the transaction date is transformed by the author of dataset to be a continuous variable). 

One year increase in the transaction **date** corresponds to the average housing price per unit to increase by 5.018 ten thousand New Taiwan Dollars, approximately 1,556 USD. One unit increase in housing **age** corresponds to the expected housing price per unit to decrease by 2568 New Taiwan Dollars. One meter increase in the **distance** to nearest metro station corresponds to the average housing price per unit to decrease by 40.45 New Taiwan Dollars. One unit increase in the number of **stores** nearby corresponds to the average housing price per unit to increase by 13200 New Taiwan Dollars. One unit increase in **latitude** corresponds to the average housing price per unit to increase by 2045000 New Taiwan Dollars, and **Longitude** an increase of 1321 New Taiwan Dollars.

After fitting the model to the testing set, the testing MSE is 60.1 and testing RMSE is 7.75.

```{r, fig.width=8, fig.height=4, echo=FALSE}
par(mfrow=c(1,2))  
plot(fit1,1)
plot(fit1,2)
```
The Residuals vs Fitted plot shows that most residuals are centered around zero, indicating that the linearity assumption holds reasonably well. However, there is some evidence of slight heteroscedasticity, as residuals tend to spread out more for higher fitted values. A few high-leverage or outlier points (e.g., observations 245 and 88) are also noticeable, which may impact model stability.

The Q-Q plot suggests that the residuals follow a roughly normal distribution in the central range. Nonetheless, deviations at the tails, particularly on the upper end, indicate heavy tails and potential right skewness. This suggests the presence of extreme positive residuals that could affect the model's assumptions.

The linearity and normality assumptions are roughly acceptable, but there are signs of mild heteroscedasticity and a few influential outliers, although we have already dropped that one outlier in price. Potential additional steps, such as outlier treatment or using robust regression methods, may improve model performance and stability.

## Relaxed Lasso Regression

```{r}
train_lasso <- train_set
test_lasso <- test_set
```

```{r}
Y <- as.matrix(train_lasso[, 'price']) 
X <- model.matrix(price ~ ., data = train_lasso)
```

```{r, fig.width=8, fig.height=4}
fit_lasso_cv <- cv.glmnet(X, Y, alpha = 1, nfolds = 10, intercept = TRUE)
plot(fit_lasso_cv)
```

```{r, include=FALSE}
coef_1se <- coef(fit_lasso_cv, s = "lambda.1se")
coef_1se <- coef_1se[which(coef_1se != 0), ]
```

After fitting a baseline model, our team fits a relaxed lasso regression model. In the relaxed lasso regression, we first use lasso regression to select a subset of variables. Then, we fit a multiple regression using only the selected variables. Our team sets s = "lambda.1se" as the penalty. The selected variables include housing age, distance to nearest MRT, number of stores nearby, and latitude of the apartment/house. 

```{r}
lasso_selected <- c("age", "distance", "storenum", "latitude")
dat_fit2 <- dplyr::select(train_lasso, all_of(c("price", lasso_selected))) 
```

```{r}
fit2 <- lm(price ~ ., dat = dat_fit2)
summary(fit2)
```

Then, we fit a multiple linear regression model using only selected variables. All variables are significant. The multiple R-squared is 0.6162, suggesting that 61.62% of the variance in housing price is explained by the model. 

One unit increase in housing **age** corresponds to the expected housing price per unit to decrease by 2590 New Taiwan Dollars. One meter increase in the **distance** to nearest metro station corresponds to the average housing price per unit to decrease by 39.01 New Taiwan Dollars. One unit increase in the number of **stores** nearby corresponds to the average housing price per unit to increase by 13540 New Taiwan Dollars. One unit increase in **latitude** corresponds to the average housing price per unit to increase by 2516000 New Taiwan Dollars. 

```{r, echo = FALSE}
predictions <- predict(fit2, newdata = test_lasso)
# Calculate errors
mse <- mean((test_lasso$price - predictions)^2)
rmse <- sqrt(mse)

cat("Testing MSE:", round(mse, 2), "\n")
cat("Testing RMSE:", round(rmse, 2), "\n")
```

After fitting the model to the testing set, the testing MSE is 60.58 and testing RMSE is 7.78.

```{r, fig.width=8, fig.height=4, echo=FALSE}
par(mfrow=c(1,2))  
plot(fit2,1)
plot(fit2,2)
```
Still, although far from perfect, the residual plot and QQ plot indicate that the linearity assumption and constant variance assumption are roughly met.

## Neural Network

To capture any potential non‐linear relationships between the predictors and housing price, we fit a feed‐forward neural network. We use the same 80% and 20% train/test data split. We scale all seven predictors (including the transaction date to capture market-wide price trends over time) to unit variance based on the training set and predict price with a network of two hidden layers (code hidden).

```{r}
library(reticulate)
np <- import("numpy")
np$random$seed(123L)
```

```{r, include=FALSE}
# Prepare the data
x_cols <- c("date", "age", "distance", "storenum", "latitude", "longitude")
y_col  <- "price"

x_train <- as.matrix(train_set[ , x_cols])
x_test  <- as.matrix(test_set[  , x_cols])
y_train <- train_set[[y_col]]
y_test  <- test_set[[y_col]]

# Scale all the data by the train‐set mean & sd
train_means <- colMeans(x_train)
train_sds   <- apply(x_train, 2, sd)
x_train <- sweep(sweep(x_train, 2, train_means, "-"), 2, train_sds, "/")
x_test  <- sweep(sweep(x_test,  2, train_means, "-"), 2, train_sds, "/")

# Split off 20% of training set as validation
val_idx <- createDataPartition(y_train, p = 0.2, list = FALSE)

x_val <- x_train[val_idx, , drop = FALSE]
y_val <- y_train[val_idx]
x_tr  <- x_train[-val_idx, , drop = FALSE]
y_tr  <- y_train[-val_idx]

x_tr_tf  <- tf$convert_to_tensor(x_tr,  dtype = tf$float32)
y_tr_tf  <- tf$convert_to_tensor(y_tr,  dtype = tf$float32)
x_val_tf <- tf$convert_to_tensor(x_val, dtype = tf$float32)
y_val_tf <- tf$convert_to_tensor(y_val, dtype = tf$float32)
```

We first selected our six predictors (transaction date, age, distance, store count, latitude, longitude) and the target (price), then standardized all features using the training‐set means and standard deviations. Then, we set aside 20% of the training data as a validation set for early‐stopping during model fitting.

```{r}
library(keras)
library(tensorflow)
tensorflow::set_random_seed(123)

# We will build and compile our model
model_nn <- keras_model_sequential(
  list(
    layer_dense(
      units       = 64,
      activation  = "relu",
      input_shape = c(ncol(x_tr))
    ),
    layer_dropout(rate = 0.3),
    layer_dense(units = 32, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 1,  activation = "linear")
  )
)

model_nn$compile(
  optimizer = optimizer_adam(learning_rate = 1e-3),
  loss      = "mse",
  metrics   = list("mean_squared_error")
)
```

For this network, we chose two hidden layers (64 $\to$ 32 units). With only around 330 training examples, we don’t want a gigantic network that overfits immediately. Two layers gives the net some ability to learn non‑linear interactions without being so deep that it memorizes the noise. 64 units in the first layer is a modest size. It is big enough to learn a variety of intermediate "features" but small enough to train quickly. Cutting down to 32 in the second layer creates a simple funnel, forcing the network to compress and distill the representation before the final prediction.

We used ReLU activations for hidden layers. We decided to randomly drop 30% of the neurons each batch to prevent co‑adaptation of units and help the model generalize better ona small dataset. Since we’re predicting a continuous price, the final layer must be able to produce any real value. A single unit with activation="linear" is suitable for this task.

```{r}
# We set up early stopping
early_stop <- callback_early_stopping(
  monitor              = "val_loss",
  patience             = 5,
  restore_best_weights = TRUE
)

# Fit the model
history_nn <- model_nn$fit(
  x               = x_tr_tf,
  y               = y_tr_tf,
  validation_data = list(x_val_tf, y_val_tf),
  epochs          = 100L,
  batch_size      = 16L,
  callbacks       = list(early_stop),
  verbose         = 0
)
```
We use early stopping in this model as by design, it will halt training once the validation loss stops improving. In a small dataset (around 330 examples), we can prevent over-fitting, avoid letting the network memorize noise once the validation loss flattens or rises.

```{r, echo=FALSE, warning=FALSE}
loss_vec     <- unlist(history_nn$history$loss)
val_loss_vec <- unlist(history_nn$history$val_loss)
n_epochs     <- length(loss_vec)
best_epoch <- which.min(history_nn$history$val_loss)

cat(sprintf(
  "Best validation loss at epoch: %d \nStopped after %d epochs\n",
  best_epoch, 
  n_epochs
))
```

```{r, echo=FALSE, warning=FALSE}
x_test_tf <- tf$convert_to_tensor(x_test, dtype = tf$float32)
y_test_tf <- tf$convert_to_tensor(y_test, dtype = tf$float32)

# Evaluate on held‐out test set
eval_res <- model_nn$evaluate(
  x_test_tf,
  y_test_tf,
  verbose = 0
)
mse  <- as.numeric(eval_res[[1]])
rmse <- sqrt(mse)

cat(sprintf(
  "Test MSE:  %.4f\nTest RMSE: %.4f\n",
  mse, rmse
))
```

The model’s validation loss stopped by epoch 90. Epoch 85 with best validation loss shows that the network had extracted the most useful patterns from the data without yet beginning to overfit. The model continued to improve until epoch 85, where further training no longer yielded lower validation MSE. After epoch 85, validation loss stopped improving, so any further training simply fit noise rather than signal. 

For this NN model, the Test MSE is 40.4118 and Test RMSE is 6.3570. To interpret these terms, our network’s predictions are off by about $\pm$ 6.36 units of price (on average). Given the baseline linear models gave RMSE $\approx$ 7.75 (vanilla OLS) and $\approx$ 7.78 (relaxed lasso), so the NN model has improved the average error by over 1 unit.

```{r, echo=FALSE, warning=FALSE, fig.width=5.5, fig.height=3}
# Plot training & validation loss
library(ggplot2)
library(tidyr)

loss_vec     <- unlist(history_nn$history$loss)
val_loss_vec <- unlist(history_nn$history$val_loss)

df <- data.frame(
  epoch    = seq_along(loss_vec),
  Training     = loss_vec,
  Validation = val_loss_vec
)

df_long <- pivot_longer(
  df,
  cols      = c(Training, Validation),
  names_to  = "set",
  values_to = "mse"
)

# Plot
ggplot(df_long, aes(x = epoch, y = mse, color = set)) +
  geom_line(size = 1) +
  labs(
    title = "Neural Net Training vs. Validation Loss",
    x     = "Epoch",
    y     = "Mean Squared Error",
    color = ""
  ) +
  theme_minimal()

```

This plot tracks mean squared error on both the training set and the held‑out validation set over 90  epochs. In the first 10–12 epochs, MSE plunges dramatically—from roughly 1,500 down to about 100 on training and around 80 on validation—as the network quickly learns the main signal. After that point, training MSE fluctuates around 100–150 while validation MSE continues a gentle decline toward 30–50, showing that additional epochs still yield minor gains on unseen data. The small and stable gap between the two curves indicates good generalization: the model is neither under nor over‑fitting in any significant way.

```{r, warning=FALSE, results = 'hide', include=FALSE}
# Pre‑convert the fixed test set to a TensorFlow tensor
x_test_tf <- tf$convert_to_tensor(x_test, dtype = tf$float32)

# baseline RMSE
pred0   <- as.numeric(model_nn$predict(x_test_tf))
rmse0   <- sqrt(mean((pred0 - y_test)^2))

# permutation importance
vars   <- colnames(x_test)
imp_df <- tibble(feature = vars, rmse_drop = numeric(length(vars)))

for(i in seq_along(vars)) {
  xt_perm     <- x_test
  xt_perm[, i] <- sample(xt_perm[, i])
  
  xt_perm_tf  <- tf$convert_to_tensor(xt_perm, dtype = tf$float32)
  
  pred_perm   <- as.numeric(model_nn$predict(xt_perm_tf))
  rmse_perm   <- sqrt(mean((pred_perm - y_test)^2))
  
  imp_df$rmse_drop[i] <- rmse_perm - rmse0
}
```

```{r, echo=FALSE}
# rank them
imp_df %>% arrange(desc(rmse_drop))
```

This permutation importance table measures “how much worse” our final model gets when we destroy each feature’s information. We first evaluate our trained neural net on the untouched test set and record its RMSE. For each feature, we then in turn randomly shuffle its values across all test observations, while leaving all the other features in their original order. We then rerun the model to compute a new RMSE. From the table, we see that distance to MRT is by far the single most critical predictor: scrambling it inflates RMSE by 8.40 points. Age and latitude also matter, with a still strong but less effect ($\approx$ 1.35-1.86 points). Longitude has a modest effect ($\approx$ 0.93 RMSE points each). Transaction date and Storenum have a very small effect ($\approx$ 0.36-0.38 points). 

In short, accessibility and basic spatial factors drive price predictions, whereas temporal trends and local amenity counts matter much less.

```{r, include=FALSE, results='hide'}
pred_nn  <- as.numeric(model_nn$predict(x_test_tf))
residual <- pred_nn - y_test
```
```{r, echo=FALSE, fig.width=6, fig.height=3}
plot(pred_nn, residual,
     xlab = "Predicted Price", ylab = "Residuals")
abline(h = 0, lty = 2)
```

Each point shows one test observation’s prediction error (predicted - actual) plotted against its predicted price. The errors cluster tightly around zero with no obvious funnel or curve. This means that the model doesn’t systematically over or under‑predict at any price level. It also satisfies homoscedasticity. There’s also no residual trend, so the model appears to have captured the main signal and left only random noise. 

```{r, echo=FALSE, fig.width=5, fig.height=3}
library(ggplot2)

df_pred <- data.frame(
  Actual    = y_test,
  Predicted = pred_nn
)

# compute limits
lims <- range(c(df_pred$Actual, df_pred$Predicted))
low  <- floor(lims[1] / 10) * 10
high <- ceiling(lims[2] / 10) * 10

ggplot(df_pred, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "#2c3e50", size = 2) +
  geom_abline(intercept = 0, slope = 1,
              color = "red", size = 1) +
  scale_x_continuous(
    limits = c(low, high),
    breaks = seq(low, high, by = 10)
  ) +
  scale_y_continuous(
    limits = c(low, high),
    breaks = seq(low, high, by = 10)
  ) +
  labs(
    x        = "Actual Price",
    y        = "Predicted Price"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title    = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12,
                                margin = ggplot2::margin(t = 0, r = 0, b = 10, l = 0)),
    panel.grid.minor = element_blank()
  )

```

This scatter plot compares each test observation’s actual price to the neural net’s predicted price, with the red line showing the best‑fit linear trend. Most points lie close to that line, indicating the model captures the bulk of the variation in price. The clustering around the line, combined with an out‑of‑sample RMSE of 6.4 shows that the network makes accurate, unbiased predictions for the majority of properties. Even if a few points deviate more at the extremes (under or over‑predicting rare high or low‑priced units), but overall the tight alignment confirms strong predictive performance.


## Random Forest
Let me do a search over the mtry (# of features to consider when splitting nodes) and ntree (trees in the forest) hyper-parameters to fine-tune this Random Forest.
```{r}
train_rf <- train_set
test_rf <- test_set

mtry_values <- c(2, 5, 10, 15)
ntree_values <- c(50, 100, 150, 200, 250, 300)
results <- data.frame(mtry = numeric(),
                      ntree = numeric(),
                      OOB_MSE = numeric())

set.seed(123)

# Loop over all combinations of mtry and ntree
for (m in mtry_values) {
  for (n in ntree_values) {
    # it seems that dropping the outlier help in performance
    rf_model <- randomForest(price ~ ., data = train_rf,
                             mtry = m, ntree = n)
    
    oob_mse <- rf_model$mse[n]
    results <- rbind(results,
                     data.frame(mtry = m, ntree = n, OOB_MSE = oob_mse))
  }
}

results <- results[order(results$OOB_MSE), ]
print(results)
```
```{r}
rf_fit = randomForest(price ~ ., data = train_rf,
                             mtry = 2, ntree = 150)

mse <- rf_fit$mse[rf_fit$ntree]
var_y <- var(train_rf$price)
r2 <- 1 - (mse / var_y)
print(r2)
```
The random forest model achieved an R² around **0.77**, indicating that approximately **77%** of the variance in housing prices is explained by the model. This suggests a relatively good fit, with the model successfully capturing most of the variability in the target variable. However, about **23%** of the variance remains unexplained, possibly due to noise or un-processed predictors.
```{r, echo=FALSE}
test_pred <- predict(rf_fit, newdata = test_rf)
residuals <- test_rf$price - test_pred

mse_test <- mean(residuals^2)
rmse_test <- sqrt(mse_test)

cat("Test MSE:", round(mse_test), "\n")
cat("Test RMSE:", round(rmse_test, 2), "\n")
```
```{r, echo=FALSE}
cat("Mean Test RF Price:", mean(test_rf$price), "\n")
cat("Median Test RF Price:", median(test_rf$price), "\n")
cat("Test RF Price SD:", sd(test_rf$price), "\n")
```
The MSE on the test set is approximately **35**, and the corresponding RMSE is around **6**. Given that the **average housing price** in the test set is around **38**, the RMSE represents around **14%** of the mean housing price (6 / 38 $\approx$ 0.15-0.16). This suggests a relatively moderate prediction error relative to the target variable's scale.

The standard deviation of housing prices in the test set is around **12**, indicating a fairly wide spread of housing prices. An RMSE of 5.86 compared to an SD of 12.29 implies that the model captures a reasonable amount of the variance in the data but still leaves room for improvement, especially for extreme cases or outliers.

Overall, the random forest model demonstrates a **good fit** with an **R² of 0.77** in the training set and maintains **acceptable generalization performance** on the test set.

Let me dive into parameter importance to better understand the contribution of the these features, which could also be a possible solution of such a relatively low fit.
```{r, fig.width=5, fig.height=3.5}
varImpPlot(rf_fit)
```
```{r, fig.width=7, fig.height=3}
# run it several times to see if the results agree
#set.seed(123)
rownum <- sample(1:nrow(X), 1)

X <- train_rf[, -which(names(train_rf) == "price")]
y <- train_rf$price
predictor <- Predictor$new(rf_fit, data = X, y = y)
shap <- Shapley$new(predictor, x.interest = X[rownum, ])
plot(shap)
```

It seems that these two feature importance methods agree that **date** sometimes is not a good contributor to the prediction of the price in Taiwan, but this is not always the case when we try to randomly sample cases for SHAP visualization, which is predictable given a skewed dataset distribution. 

The variability in the SHAP values for the **date** variable suggests that its influence depends heavily on the specific characteristics of the sampled cases. Additionally, the skewness in the dataset can amplify such inconsistencies, as random samples might capture outlier patterns or regions where the effect of **date** is either exaggerated or diminished, highlighting a need for data pre-processing to convert the distribution of input to a more normal shape.

Also, for instance, in some observations, the transaction date may correlate with market trends or seasonal effects that impact housing prices, while in other cases, it may provide little explanatory power. This inconsistency is expected in our datasets where only six different independent variables were recorded, indicating that our input data may needs some extra data to be collected to have a consistent feature importance. 



## Conclusion
| Model | MSE | RMSE | Strength | Weakness |
|-------|-------|-------|-------|-------|
| Linear Regression | 60.1 | 7.75 | Very interpretable | Underfits nonlinearity |
| Relaxed LASSO | 60.58 | 7.78 | Sparse and interpretable | Still linear |
| Neural Network | 40.4118 | 6.3570 | Captures nonlinear patterns | Harder to interpret |
| Random Forest | 35 | 6 | Predicting Power | Black box |

Across our four models, predictive performance steadily improved as we moved from simple linear methods to more flexible, non‐linear approaches. The vanilla OLS baseline yielded a test MSE of 61.09. Adding relaxed LASSO variable selection trimmed this only slightly (MSE = 60.7), indicating minimal loss from dropping longitude and date. Our two‐layer neural network then drove MSE down to 41.45, leveraging non‐linear interactions and modest dropout to reduce average error by over one unit compared to OLS. Finally, a random forest (with optimal mtry and ntree) achieved comparable gains (MSE = 35), due to its ensembling of regression trees that capture complex spatial and age–distance effects without extensive tuning. In strength, the neural net and forest both harness non‐linearities and interactions that linear models miss. However, the NN requires careful monitoring (early stopping, dropout) to avoid overfitting, and the RF, while robust, can be harder to interpret and slower to score. In contrast, OLS and LASSO are transparent, fast, and easy to deploy but leave substantial unexplained variance on the table.
