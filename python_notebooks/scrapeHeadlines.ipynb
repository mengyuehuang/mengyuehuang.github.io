{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hYhsM7084cd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "from fake_useragent import UserAgent\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WNntIXA84ce"
      },
      "source": [
        "# We split the data into 3 parts so that each of our group members can run it to create \"manual parallelism.\" If we do actual parallelism we run into issues with the rate limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51SQbjE884cf"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "# commented out to prevent accidental execution #\n",
        "######################################################\n",
        "\n",
        "\n",
        "# split data into 3 equal parts\n",
        "\n",
        "# df = pd.read_csv(\"url_only_data.csv\")\n",
        "# df1 = df.iloc[:len(df)//3]\n",
        "# df2 = df.iloc[len(df)//3:2*len(df)//3]\n",
        "# df3 = df.iloc[2*len(df)//3:]\n",
        "# # Save each part to a separate CSV file\n",
        "# df1.to_csv(\"url_only_data_part1.csv\", index=False)\n",
        "# df2.to_csv(\"url_only_data_part2.csv\", index=False)\n",
        "# df3.to_csv(\"url_only_data_part3.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QgPrVAY84cf"
      },
      "outputs": [],
      "source": [
        "# Load URLs\n",
        "df = pd.read_csv(\"url_only_data_part3.csv\")\n",
        "urls = df['url'].dropna().tolist()\n",
        "\n",
        "# Scrape headlines\n",
        "def extract_headline(url):\n",
        "    ua = UserAgent()\n",
        "    headers = {\n",
        "        \"User-Agent\": ua.random,\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "    }\n",
        "    try:\n",
        "        session = requests.Session()\n",
        "        session.headers.update(headers)\n",
        "        resp = session.get(url)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "\n",
        "        # Try <h1>, <title>, <h2>, and meta tags\n",
        "        for tag in ['h1', 'title', 'h2', 'meta']:\n",
        "            if tag == 'meta':\n",
        "                meta = soup.find('meta', property='og:title') or soup.find('meta', attrs={'name': 'title'})\n",
        "                if meta and meta.get('content'):\n",
        "                    return meta['content'], None\n",
        "            else:\n",
        "                tag_result = soup.find(tag)\n",
        "                if tag_result and tag_result.get_text(strip=True):\n",
        "                    return tag_result.get_text(strip=True), None\n",
        "\n",
        "        return '', 'Headline not found'\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "        return None, str(e)\n",
        "\n",
        "results = []\n",
        "for url in tqdm(urls):\n",
        "    headline, error = extract_headline(url)\n",
        "    results.append({'url': url, 'headline': headline, 'error': error})\n",
        "    delay = random.random()\n",
        "    time.sleep(delay)\n",
        "\n",
        "# Save and view\n",
        "result_df = pd.DataFrame(results)\n",
        "result_df.to_csv(\"headline_results_3.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mrOmQoN84cg"
      },
      "source": [
        "# Here we combine the results and validate the headlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKKNn7AR84cg"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"headline_results1.csv\")\n",
        "df2 = pd.read_csv(\"headline_results2.csv\")\n",
        "df3 = pd.read_csv(\"headline_results3.csv\")\n",
        "# Combine the three DataFrames\n",
        "combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv(\"headlines.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LrYu67484cg",
        "outputId": "eb05bbdb-c6fb-46cd-b049-29b4b4814340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "there are 3 nans in the headline column\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2323    500 Server Error: Internal Server Error for ur...\n",
              "2354    404 Client Error: Not Found for url: https://w...\n",
              "3092    500 Server Error: Internal Server Error for ur...\n",
              "Name: error, dtype: object"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df = pd.read_csv(\"headlines.csv\")\n",
        "# print number of nans in the headline column\n",
        "print(f\"there are {final_df['headline'].isna().sum()} nans in the headline column\")\n",
        "nan_error = final_df[final_df['headline'].isna()][\"error\"]\n",
        "nan_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oavHSKR-84ch"
      },
      "source": [
        "These are server side errors. We can't do anything about them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7c834Uq84ch"
      },
      "source": [
        "# Here we extract the news outlets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5qCR1Al84ch",
        "outputId": "5ab9023d-5038-4742-db54-1645e653274e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "there are 0 nans in the outlet column\n"
          ]
        }
      ],
      "source": [
        "final_df['outlet'] = final_df['url'].str.extract(r'https?://(?:www\\.)?([\\w\\-]+)\\.com')\n",
        "# print number of nans in the outlet column\n",
        "print(f\"there are {final_df['outlet'].isna().sum()} nans in the outlet column\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqTYe3tc84ch"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv(\"headlines_and_outlet.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "native",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}